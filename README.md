# Summary
This report focuses on how certain characteristics and habits of students may or may not influence a student's grades. Utilizing a dataset outlining a variety of personal, educational, and familial features of a student along with their final grade ranked from 0-7, our objective is to create a predictive model for student grades.  With this data in mind, we were able to tackle the following overarching research questions: as students, are our grades in our own control, (or are they predetermined by our circumstances i.e. gender), and which group of features is most influential on a studentâ€™s grade (options: personal, educational, or familial). The machine learning programs applied to the data included a random forest regressor (to understand which features are most important in determining a student's grade) and k-nearest neighbor classifiers (to determine which category of features bears the most influence on grades by comparing the accuracy of the classifiers when predicting student grade). 

## Discussion

When exploring the basic patterns in the data, our initial expectation for strong relationships between student features like hours of study and frequency of notes was debunked as seen in the correlation heatmap. these features actually had the lowest correlation calculated in the heatmap. Cumulative GPA having a strong relationship to a student's grade was something expected but an unexpected finding was the strong correlation with the gender of the student. However, when examining gender distribution for the highest and lowest grades, the visualizations helped reveal strong bias in the original dataset as the small sample size depicted only women receiving failing grades and only men achieving the highest grades. Looking at the PCA map, we expected clear groupings of points based on grades when compressing the features into two dimensions, yet no clear groups appeared. This leads us to believe that culmination of student features was not a clear indicator of the grade a student received. 

Our hypothesis of student features being able to predict a student's grade was refuted with the results of our machine learning tools at our disposal. Both a regressor and a classifier were utilized to interpret whether treating a grade as a class or a continuous number would improve results. This assumption was disproven as each model proved to be equally unfit for predictive purposes.  

In the k-NN classifier, each feature segmented classifier depicted very low accuracies. These accuracies reflect the k-NN classifier was not well suited to provide students or those educational field insights into being able to give more realistic expectations for an individual's grade. One takeaway that could be gathered from comparing the accuracies is that educational factors play the greatest role in predicting classifiers. However, that conclusion should be qualified by the fact that the educational classifier included the most features (14 as opposed to 10 in the personal classifier or 7 features in the familial classifier). More features can often improve a classifier's performance and the features included are some of the most highly correlated to a student's grade (expected and cumulative GPA). With this in mind, perhaps other features about other student behavior and characteristics would be better suited to predict grades. Additionally, often the most accurately classified grade was grade 1 because the highest number of samples were present in that category (refer to sample weighting from random forest regressor), so this was another factor to consider when qualifying these results. 


In the random forest regressor, a similar rejection of our hypothesis occurred. The regressor also failed to provide substantial proof that a predictive model based on student attributes could be feasible and accurate. Low cross-validated R^2 scores indicated that our model would not be able to predict a new student's grade accurately as a very weak relationship was detected between grades and student features. The consistency of the features of importance and nonimportance with the correlation heatmap leads us to believe that perhaps the regressor discerned the correct patterns in the data, but the student grades can not be predicted with the present student features in the data. 


We were not able to make a conclusive data science - backed answer to the overarching question about whether grades are in a student's control or are determined by pre-determined factors. Our project and its failure to provide productive outputs can be traced back to many factors. For instance, the grade being reported discretely in this dataset limited the machine learning methods at our disposal such as linear regression and other methods that fit better with continuous features. Additionally, the results from a very small sample did not help as our methods have less data to train on. Even if the results were depicted as accurate for each model, these results could not be applicable to a variety of settings. The small dataset focused on a set of students in the same department of the same school in the same location. This falls short of accounting for geographical and regional differences, differing university standards and rigor, and differing class requirements. With so many factors at play for a student's grades, our lack of functional models begs us to ask the question: is a student's grade beyond the ability of prediction with the many different conditions and human behavior at play?

